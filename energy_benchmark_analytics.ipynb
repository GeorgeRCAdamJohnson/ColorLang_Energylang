{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf86060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8f8502",
   "metadata": {},
   "source": [
    "## Database Connection\n",
    "\n",
    "Set up the connection to your PostgreSQL database using SQLAlchemy for easy DataFrame loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10891390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update with your actual credentials or use environment variables\n",
    "import os\n",
    "DB_URL = os.getenv('DATABASE_URL', 'postgresql://postgres:YOUR_PASSWORD@localhost:5432/energy_lang')\n",
    "engine = create_engine(DB_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7b3a31",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Load the `sources`, `benchmarks`, and `results` tables into pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f046a4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = pd.read_sql('SELECT * FROM sources', engine)\n",
    "benchmarks = pd.read_sql('SELECT * FROM benchmarks', engine)\n",
    "results = pd.read_sql('SELECT * FROM results', engine)\n",
    "\n",
    "print(f'Sources: {len(sources)} rows')\n",
    "print(f'Benchmarks: {len(benchmarks)} rows')\n",
    "print(f'Results: {len(results)} rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864394e1",
   "metadata": {},
   "source": [
    "## Data Integrity Checks\n",
    "\n",
    "Check for duplicates, missing values, and orphaned records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b3cce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate sources\n",
    "print('Duplicate sources:', sources.duplicated(['name']).sum())\n",
    "\n",
    "# Check for duplicate benchmarks (by unique key)\n",
    "benchmarks_key = ['source_id', 'test_name', 'language', 'toolchain', 'version', 'workload']\n",
    "print('Duplicate benchmarks:', benchmarks.duplicated(benchmarks_key).sum())\n",
    "\n",
    "# Orphaned benchmarks (no matching source)\n",
    "orphaned_benchmarks = benchmarks[~benchmarks['source_id'].isin(sources['id'])]\n",
    "print('Orphaned benchmarks:', len(orphaned_benchmarks))\n",
    "\n",
    "# Orphaned results (no matching benchmark)\n",
    "orphaned_results = results[~results['benchmark_id'].isin(benchmarks['id'])]\n",
    "print('Orphaned results:', len(orphaned_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663ccb19",
   "metadata": {},
   "source": [
    "## Join Data for Analysis\n",
    "\n",
    "Merge the tables for richer analytics and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ac5667",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks_full = benchmarks.merge(sources, left_on='source_id', right_on='id', suffixes=('', '_source'))\n",
    "results_full = results.merge(benchmarks_full, left_on='benchmark_id', right_on='id', suffixes=('', '_bench'))\n",
    "results_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60acbddf",
   "metadata": {},
   "source": [
    "## Summary Statistics\n",
    "\n",
    "Get a quick overview of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775c21c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_full.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7d92ce",
   "metadata": {},
   "source": [
    "## Visualization: Throughput by Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b302d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(results_full, x='language', y='throughput_ops_per_sec', points='all', title='Throughput by Language')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459ce31e",
   "metadata": {},
   "source": [
    "## Visualization: Latency by Toolchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fad0285",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(results_full, x='toolchain', y='latency_ms', points='all', title='Latency by Toolchain')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1903ae8",
   "metadata": {},
   "source": [
    "## Interactive Filtering Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d787754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "language_options = results_full['language'].dropna().unique().tolist()\n",
    "language_select = widgets.Dropdown(options=language_options, description='Language:')\n",
    "\n",
    "def update_plot(language):\n",
    "    filtered = results_full[results_full['language'] == language]\n",
    "    fig = px.scatter(filtered, x='throughput_ops_per_sec', y='latency_ms', color='toolchain',\n",
    "                     hover_data=['test_name', 'version', 'workload'],\n",
    "                     title=f'Throughput vs Latency for {language}')\n",
    "    fig.show()\n",
    "\n",
    "widgets.interact(update_plot, language=language_select);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c0c710",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Add more visualizations (energy, power, trends over time)\n",
    "- Export summary tables or charts\n",
    "- Integrate with Streamlit or Dash for a web dashboard if needed"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
